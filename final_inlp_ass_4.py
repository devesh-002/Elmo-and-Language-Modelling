# -*- coding: utf-8 -*-
"""final_iNLP_ass_4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U9x8c4z1GKvPhkzfOB8aHQES-6e-OrBi
"""

from google.colab import drive
drive.mount('/content/drive')
!pip install datasets
!pip install PyStemmer
!pip install scikit-plot

import nltk
import torchtext
import torch
import torch.nn as nn
import os
import sys
import numpy as np
## imports 
import regex as re
import pandas as pd
import torch.nn.functional as F
import torch.optim as optim
from nltk.tokenize import word_tokenize

import os
from datasets import load_dataset
from nltk.corpus import stopwords
from Stemmer import Stemmer
from torchtext import vocab
from tqdm import tqdm
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print('Device available now:', device)
from torch.utils.data import Dataset, DataLoader
import string

stemmer = Stemmer('porter')
try:
    nltk.download("stopwords")
    nltk.download("punkt")
except:
    pass
stop_words = set(stopwords.words("english"))

sst_dataset = load_dataset("sst", "default")
nli_dataset = load_dataset("multi_nli")

print(sst_dataset)

print(sst_dataset['train']['tokens'][0])

def tokenizer( wiki):
  # wiki = re.sub(r'[^\x00-\x7F]+', ' ', wiki)
  # wiki = re.sub('&amp;|&apos;|&gt;|&lt;|&nbsp;|&quot;', ' ', wiki)
  # wiki = re.split(r"[^A-Za-z0-9]+", wiki)
  wiki = wiki.translate(str.maketrans('', '', string.punctuation))
  wiki = word_tokenize(wiki)
  wiki = [word for word in wiki if not re.match(r'\d+', word) and len(word) > 1]

  wiki = [stemmer.stemWord(w.lower())for w in wiki if not w.lower() in stop_words and len(w) > 1]
  # wiki = wiki.split()
  return wiki
def setup_labels(data,id_list=['train','test','validation']):
  l=[]
  for id in id_list:

    for label in data[id]['label']:
      if(label>0.5):
       l.append(1)
      else:
        l.append(0)
  return np.array(l)

labels_sst=setup_labels(sst_dataset,id_list=['train'])
labels_sst_test=setup_labels(sst_dataset,id_list=['test'])
labels_sst_valid=setup_labels(sst_dataset,id_list=['validation'])

def sst_tokeniser(data,min_freq=5,id_list=['train','test','validation']):
  list_=[]
  d={}
  total_words=set()
  for id in id_list:
    tokens=data[id]['sentence']

    for tok in tokens:
      # print(tok)
      x=tokenizer(tok)
      list_.append(x)
      for word in x:
        if word in d:
          d[word]+=1
        else:
          d[word]=1
  vocablary = vocab.vocab(d,min_freq = min_freq,specials = ['<UNK>', '<PAD>'])

  vocablary.set_default_index(vocablary['<UNK>'])
  return list_,total_words,vocablary,d
  

    
list_sst,total_words_sst,vocabulary_sst,index_dict_sst=sst_tokeniser(sst_dataset)
print(len(vocabulary_sst))

list_sst_test,total_words_sst_test,vocabulary_sst_test,index_dict_sst_test=sst_tokeniser(sst_dataset,id_list=['test'])
list_sst_valid,total_words_sst_valid,vocabulary_sst_valid,index_dict_sst_valid=sst_tokeniser(sst_dataset,id_list=['validation'])

list_sst_train,_,_,_=sst_tokeniser(sst_dataset,id_list=['train'])

def get_embedding_layer( vocabulary, embedding_dim):
        glove_vectors = vocab.GloVe(name = '6B', dim = embedding_dim)
        UNK_EMB = torch.mean(glove_vectors.vectors, dim=0)
        embeddings = []
        for word in vocabulary.get_itos():
            if word in glove_vectors.itos:
              embeddings.append(glove_vectors[word])
            else: 
              embeddings.append(UNK_EMB)
        embeddings = torch.stack(embeddings)
        embedding = nn.Embedding.from_pretrained(embeddings, freeze=True, padding_idx=vocabulary['<PAD>'])
        return embedding
embedding_sst=get_embedding_layer(vocabulary_sst,300)

print(len(list_sst))

# create dataset
          # x_padded_new=[0]*(maxlen-1-i)
          # x_padded_new+=seq[:i]
          # x.append(x_padded_new)
          # y.append(w)
def make_target_dataset(data,vocab):
  input=[]
  target=[]
  for i,sentence in enumerate(data):
    l=[]
    for w_idx in range(0,len(sentence)):
      word=sentence[w_idx]
      l.append(vocab([word])[0])
    input.append(l.copy())
    sentence.reverse()
    # l=[]
    # for w_idx in range(0,len(sentence)):
    #   word=sentence[w_idx]
    #   l.append(vocab([word])[0])
    # input.append(l.copy())

    # l=[]
    # for w_idx in range(len(sentence)-1,-1,-1):
    #   word=sentence[w_idx]
    #   l.append(vocab([word])[0])
    # input.append(l.copy())

  # print(input.shape,target.shape)

  return input
    
input_sst_util=make_target_dataset(list_sst,vocabulary_sst)

input_sst_util_train=make_target_dataset(list_sst_train,vocabulary_sst)

def prepare_sentence(seq, maxlen):
    # Pads seq and slides windows
    x = []
    y = []
    for i, w in enumerate(seq):
          # print(i)
          # print([seq[:i]])
          x_padded_new=[0]*(maxlen-1-i)
          x_padded_new+=seq[:i]
          x.append(x_padded_new)
          y.append(w)
    
    return x, y
def final_prep(tokenised_sequence):
  maxlen = max([len(seq) for seq in tokenised_sequence])
  x = []
  y = []
  for seq in tokenised_sequence:
      x_windows, y_windows = prepare_sentence(seq, maxlen)
      x += x_windows
      y += y_windows
  return torch.tensor(np.array(x)),torch.tensor(np.array(y))
input_sst,target_sst=final_prep(input_sst_util_train)

def prepare_sentence_(seq, maxlen):
    # Pads seq and slides windows
    x = []
    y = []
    length=len(seq)
    # for i, w in enumerate(seq):
    x_padded_new=[0]*(maxlen-length)
    x_padded_new+=seq[:length]
    return [x_padded_new]
def final_prep_(tokenised_sequence):
  maxlen = max([len(seq) for seq in tokenised_sequence])
  x = []
  y = []
  for seq in tokenised_sequence:
      x_windows = prepare_sentence_(seq, maxlen)
      # print(len(x_windows[0]))
      x += x_windows
  return torch.from_numpy(np.array(x))
sentence_sst=final_prep_(input_sst_util_train)

input_sst_util_test=make_target_dataset(list_sst_test,vocabulary_sst_test)
input_sst_test,target_sst_test=final_prep(input_sst_util_test)
sentence_sst_test=final_prep_(input_sst_util_test)

input_sst_util_valid=make_target_dataset(list_sst_valid,vocabulary_sst_valid)
input_sst_valid,target_sst_valid=final_prep(input_sst_util_valid)
sentence_sst_valid=final_prep_(input_sst_util_valid)

"""### nli"""

print(nli_dataset)

def nli_tokeniser(data,min_freq=5,id_list=['train','validation_matched','validation_mismatched']):
  list_=[]
  d={}
  label_nli=[]
  total_words=set()
  i=0
  for id in id_list:
    tokens=data[id]['premise']
    labels=data[id]['label']

    for tok,label in zip(tokens,labels):
      # sent=" ".join(tok.split('|'))
      if(label==-1):
        continue
      label_nli.append(label)
      # print(sent)
      x=tokenizer(tok)
      list_.append(x)
      for word in x:
        if word in d:
          d[word]+=1
        else:
          d[word]=1
      i+=1
      if(i>=40000):
        break
    if(i>=40000):
      break
  vocablary = vocab.vocab(d,min_freq = min_freq,specials = ['<UNK>', '<PAD>'])

  vocablary.set_default_index(vocablary['<UNK>'])
  return list_,total_words,vocablary,d,np.array(label_nli)

list_nli,total_words_nli,vocabulary_nli,index_dict_nli,label_nli=nli_tokeniser(nli_dataset)
list_nli_test,total_words_nli_test,vocabulary_nli_test,index_dict_nli_test,label_nli_test=nli_tokeniser(nli_dataset,id_list=['validation_matched'])
list_nli_valid,total_words_nli_valid,vocabulary_nli_valid,index_dict_nli_valid,label_nli_valid=nli_tokeniser(nli_dataset,id_list=['validation_mismatched'])

input_nli_util=make_target_dataset(list_nli,vocabulary_nli)
input_nli,target_nli=final_prep(input_nli_util)
sentence_nli=final_prep_(input_nli_util)

input_nli_util_test=make_target_dataset(list_nli_test,vocabulary_nli)
input_nli_test,target_nli_test=final_prep(input_nli_util_test)
sentence_nli_test=final_prep_(input_nli_util_test)

input_nli_util_valid=make_target_dataset(list_nli_valid,vocabulary_nli)
input_nli_valid,target_nli_valid=final_prep(input_nli_util_valid)
sentence_nli_valid=final_prep_(input_nli_util_valid)

embedding_nli=get_embedding_layer(vocabulary_nli,300)

"""### final elmo"""

BATCH_SIZE=128
class Elmo_bs(nn.Module):
    def __init__(self, vocab, embeddings, hid_size=300,PAD_VAL=0,num_layers=2):
        super().__init__()
        self.lstm = nn.LSTM(hid_size, hid_size, num_layers = num_layers, bidirectional=True, batch_first = True)
        self.pad = PAD_VAL
        # self.labels_num = 2 # for sst
        self.labels_num = 3 # for NLI

        self.embedding = embeddings
        self.weights = torch.rand(2)

        self.word_pred = nn.Linear(hid_size*2, len(vocab))
        self.classifier = nn.Linear(hid_size*2, self.labels_num)
        self.layers =num_layers
        self.softmax = nn.Softmax(dim=0)
        
    def forward(self, x):
        embed = self.embedding(x)

        a, (y, _) = self.lstm(embed)
        shape = y.shape
        y = torch.reshape(y,(shape[1],shape[0],shape[2]))
        y = torch.reshape(y,(shape[1], int(shape[0]/self.layers), self.layers, shape[2]))

        a = self.softmax(self.weights)
        a=torch.unsqueeze(a,1)
        a = torch.unsqueeze(torch.unsqueeze(a,0),0)
        a = a.repeat(shape[1], 2, 1, shape[2]).to(device)
        
        y = torch.mul(y, a)
        y = torch.sum(y, dim=2)
        y = torch.flatten(y,1,2)
        return y


"""### try"""

model_sst=Elmo_bs(vocabulary_sst,embedding_sst).to(device)

optimizer_sst = optim.Adam(model_sst.parameters(),lr=0.001,weight_decay=1e-4)
criterion_sst = nn.CrossEntropyLoss(ignore_index = vocabulary_sst['<PAD>']).to(device)

def train(model,criterion,optim,batch_size,input_data,target_data,valid_in_data,valid_out_data,EPOCHS=25):

    min_valid_loss=1e9
    inp_dataloader=DataLoader(input_data,shuffle=False,  batch_size=batch_size)
    out_dataloader=DataLoader(target_data,shuffle=False,  batch_size=batch_size)
    valid_in_loader=DataLoader(valid_in_data,shuffle=False,  batch_size=batch_size)
    valid_out_loader=DataLoader(valid_out_data,shuffle=False,  batch_size=batch_size)

    for epoch in range(EPOCHS):
      model.train()
      epoch_loss = 0
      epoch_acc = 0

      for idx,data in enumerate(zip(inp_dataloader,out_dataloader)):
        x,y=data
        inputs = x.to(device)
        targets = y.to(device)
        # print(targets)

        outputs = model(inputs)
        outputs = model.word_pred(outputs)

        loss = criterion(outputs, targets)

        optim.zero_grad()
        loss.backward()
        optim.step()

        epoch_loss += loss.item()

      epoch_loss = epoch_loss / len(input_data)
      print(f'Epoch {epoch}/{EPOCHS}\t Loss {epoch_loss}')
      valid_loss = 0.0
      model.eval()     # Optional when not using Model Specific layer
      for i, d in enumerate(zip(valid_in_loader,valid_out_loader)):
          data,labels=d
          if torch.cuda.is_available():
              data, labels = data.cuda(), labels.cuda()
          
          target = model(data)
          loss = criterion(target,labels)
          valid_loss = loss.item() * data.size(0)

      if min_valid_loss > valid_loss and epoch>30:
          print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \t Saving The Model')
          min_valid_loss = valid_loss
          # Saving State Dict
          torch.save(model.state_dict(), '/content/saved_model.pth')

    return model
model_sst=train(model_sst,criterion_sst,optimizer_sst,BATCH_SIZE,input_sst,target_sst,input_sst_valid,target_sst_valid)



# PATH="/content/drive/MyDrive/model_pretrained_sst.pth"
# torch.save(model_sst.state_dict(), PATH)

def train_pipe(model,criterion,optim,batch_size,input_data,target_data,valid_in_data,valid_out_data,EPOCHS=50):
    min_valid_loss=1e9
    inp_dataloader=DataLoader(input_data,shuffle=False,  batch_size=batch_size)
    out_dataloader=DataLoader(target_data,shuffle=False,  batch_size=batch_size)

    valid_in_loader=DataLoader(valid_in_data,shuffle=False,  batch_size=batch_size)
    valid_out_loader=DataLoader(valid_out_data,shuffle=False,  batch_size=batch_size)

    for epoch in range(EPOCHS):
      model.train()
      epoch_loss = 0
      epoch_acc = 0

      # print(len(input_data))
      # print(len(target_data))
      for idx,data in enumerate(zip(inp_dataloader,out_dataloader)):
        x,y=data
        inputs = x.to(device)
        targets = y.squeeze().to(device)
        optim.zero_grad()

        outputs = model(inputs)
        outputs = model.classifier(outputs)

        loss = criterion(outputs, targets)
        # backward pass
        loss.backward()
        optim.step()

        epoch_loss += loss.item()

      epoch_loss = epoch_loss / len(input_data)
      print(f'Epoch {epoch}/{EPOCHS}\t Loss {epoch_loss}')
      valid_loss = 0.0
      model.eval()     # Optional when not using Model Specific layer
      for i, d in enumerate(zip(valid_in_loader,valid_out_loader)):
          data,labels=d
          if torch.cuda.is_available():
              data, labels = data.cuda(), labels.cuda()
          target = model(data)
          labels = labels.squeeze().to(device)
          loss = criterion(target,labels)
          valid_loss = loss.item() * data.size(0)

      if min_valid_loss > valid_loss and epoch>10:
          print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \t Saving The Model')
          min_valid_loss = valid_loss

          torch.save(model.state_dict(), '/content/saved_model_fine.pth')


    return model


def train_pipeline_main(vocab,embedding,PATH):
  model=Elmo_bs(vocab,embedding).to(device)
  model.load_state_dict(torch.load(PATH))
  for param in model.lstm.parameters():
    param.requires_grad = False
  print(model)
  optimizer = optim.Adam(model.parameters(),lr=0.0001,weight_decay=1e-4)
  criterion = nn.CrossEntropyLoss().to(device)

  model_finetuned=train_pipe(model,criterion,optimizer,BATCH_SIZE,sentence_sst,labels_sst,sentence_sst_valid,labels_sst_valid)
  return model_finetuned

model_sst_finetuned=train_pipeline_main(vocabulary_sst,embedding_sst,"/content/saved_model_final_pretrain_53.pth")

PATH="/content/saved_model_final_53.pth"
elmo_s=Elmo_bs(vocabulary_sst,embedding_sst).to(device)
elmo_s.load_state_dict(torch.load(PATH))

from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

def get_stats(model, input_data,target_data,batch_size=BATCH_SIZE):
    true_labels = []
    predictions = []
    inp_dataloader=DataLoader(input_data,shuffle=False,  batch_size=batch_size)
    out_dataloader=DataLoader(target_data,shuffle=False,  batch_size=batch_size)
    model.eval()
    for i,batch in enumerate(zip(inp_dataloader,out_dataloader)):
        x, label = batch
        x = x.to(device)
        true_labels += label.tolist()
        pred_util = model.classifier(model(x))
        pred_util = nn.functional.softmax(pred_util, dim=1)
        pred_util = pred_util.argmax(dim=1)
        predictions += pred_util.tolist()
    print(predictions)
    model_labels=model.labels_num
    label_l=[]
    for i in range(model_labels):
      label_l.append(i)
    print('Classification Report:')
    print(classification_report(true_labels, predictions))
    print('\nConfusion Matrix:')
    cm = confusion_matrix(target_data, predictions, labels=label_l)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm,  display_labels=label_l)
    disp.plot()
    plt.show()
    return "\n"
x = get_stats(elmo_s, sentence_sst_test,labels_sst_test)
x

"""### nli dataset"""

elmo_nli_pretrained=Elmo_bs(vocabulary_nli,embedding_nli).to(device)

model_nli=Elmo_bs(vocabulary_nli,embedding_nli).to(device)

optimizer_nli = optim.Adam(model_nli.parameters(),lr=0.001,weight_decay=1e-4)
criterion_nli = nn.CrossEntropyLoss(ignore_index = vocabulary_nli['<PAD>']).to(device)
model_nli=train(model_nli,criterion_nli,optimizer_nli,BATCH_SIZE,input_nli,target_nli,input_nli_valid,target_nli_valid)

# torch.save(elmo_nli_pretrained.state_dict(), '/content/saved_model_nli.pth')

elmo_n=train_pipeline_main(vocabulary_nli,embedding_nli,"/content/saved_model_nli_34.pth")

PATH="/content/saved_model_nli_34.pth"
elmo_nli_finetune=Elmo_bs(vocabulary_nli,embedding_nli).to(device)
elmo_nli_finetune.load_state_dict(torch.load(PATH))

print(label_nli_test)



x = get_stats(elmo_nli_finetune, sentence_nli_test,label_nli_test)
x

# torch.save(elmo_n.state_dict(), '/content/saved_model_nli_34.pth')

"""# ROC curve"""

import scikitplot as skplt
import matplotlib.pyplot as plt
def roc_curve(model,input_data,target_data,batch_size=BATCH_SIZE):
    y_true = []
    y_pred = []
    inp_dataloader=DataLoader(input_data,shuffle=False,  batch_size=batch_size)
    out_dataloader=DataLoader(target_data,shuffle=False,  batch_size=batch_size)
    model.eval()
    for i,batch in enumerate(zip(inp_dataloader,out_dataloader)):
        x, label = batch
        x = x.to(device)
        y_true += label.tolist()
        preds = model(x)
        preds=model.classifier(preds)
        p = nn.functional.softmax(preds, dim=1).cpu().detach().numpy().tolist()
        y_pred+=p
    skplt.metrics.plot_roc_curve(y_true, y_pred)
    plt.show()
roc_curve(elmo_n,sentence_nli_test,label_nli_test)

roc_curve(elmo_s,sentence_sst_test,labels_sst_test)

